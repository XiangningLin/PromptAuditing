#!/usr/bin/env python3
"""
æ•°æ®é›†åˆ†æå·¥å…· - åˆ†æå’Œå¯è§†åŒ–ç§å­æç¤ºè¯æ•°æ®é›†
Dataset Analysis Tool - Analyze and visualize seed prompts dataset
"""

import pandas as pd
import json
from collections import Counter, defaultdict

def analyze_seed_prompts(filename='seed_prompts.csv'):
    """åˆ†æç§å­æç¤ºè¯æ•°æ®é›†"""
    df = pd.read_csv(filename)
    
    print("\n" + "="*80)
    print("ğŸ“Š ç§å­æç¤ºè¯æ•°æ®é›†åˆ†æ / Seed Prompts Dataset Analysis")
    print("="*80)
    
    # åŸºæœ¬ç»Ÿè®¡
    print("\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ / Basic Statistics:")
    print(f"   æ€»æç¤ºè¯æ•° / Total Prompts: {len(df)}")
    print(f"   å¥½ä¾‹å­ / Good Examples: {len(df[df['label']=='good'])} ({len(df[df['label']=='good'])/len(df)*100:.1f}%)")
    print(f"   åä¾‹å­ / Bad Examples: {len(df[df['label']=='bad'])} ({len(df[df['label']=='bad'])/len(df)*100:.1f}%)")
    print(f"   åˆ†ç±»æ•° / Categories: {df['category'].nunique()}")
    print(f"   å­åˆ†ç±»æ•° / Subcategories: {df['subcategory'].nunique()}")
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    print("\nğŸ“‚ æŒ‰åˆ†ç±»ç»Ÿè®¡ / Statistics by Category:")
    print("-" * 80)
    print(f"{'Category':<35} {'Good':<8} {'Bad':<8} {'Total':<8}")
    print("-" * 80)
    
    for category in df['category'].unique():
        cat_df = df[df['category'] == category]
        good_count = len(cat_df[cat_df['label'] == 'good'])
        bad_count = len(cat_df[cat_df['label'] == 'bad'])
        total = len(cat_df)
        print(f"{category:<35} {good_count:<8} {bad_count:<8} {total:<8}")
    
    print("-" * 80)
    
    # æŒ‰å­åˆ†ç±»ç»Ÿè®¡
    print("\nğŸ“‹ æŒ‰å­åˆ†ç±»ç»Ÿè®¡ / Statistics by Subcategory:")
    print("-" * 80)
    print(f"{'Subcategory':<35} {'ID':<6} {'Good':<6} {'Bad':<6} {'Total':<6}")
    print("-" * 80)
    
    for _, row in df.groupby(['category', 'subcategory', 'standard_id']).size().reset_index(name='count').iterrows():
        subcat_df = df[(df['category'] == row['category']) & 
                       (df['subcategory'] == row['subcategory'])]
        good_count = len(subcat_df[subcat_df['label'] == 'good'])
        bad_count = len(subcat_df[subcat_df['label'] == 'bad'])
        
        # æˆªæ–­é•¿åç§°
        subcat_name = row['subcategory'][:33] + '..' if len(row['subcategory']) > 35 else row['subcategory']
        
        print(f"{subcat_name:<35} {row['standard_id']:<6} {good_count:<6} {bad_count:<6} {row['count']:<6}")
    
    print("-" * 80)
    
    # æç¤ºè¯é•¿åº¦åˆ†æ
    print("\nğŸ“ æç¤ºè¯é•¿åº¦åˆ†æ / Prompt Length Analysis:")
    df['length'] = df['prompt'].str.len()
    df['word_count'] = df['prompt'].str.split().str.len()
    
    print(f"   å¹³å‡å­—ç¬¦æ•° / Avg Characters: {df['length'].mean():.0f}")
    print(f"   å¹³å‡è¯æ•° / Avg Words: {df['word_count'].mean():.0f}")
    print(f"   æœ€çŸ­ / Min: {df['length'].min()} å­—ç¬¦")
    print(f"   æœ€é•¿ / Max: {df['length'].max()} å­—ç¬¦")
    
    print("\n   æŒ‰æ ‡ç­¾åˆ†ç»„ / By Label:")
    for label in ['good', 'bad']:
        label_df = df[df['label'] == label]
        print(f"   {label.upper():<6} - å¹³å‡: {label_df['length'].mean():.0f} å­—ç¬¦, {label_df['word_count'].mean():.0f} è¯")
    
    return df

def analyze_generated_prompts(filename='generated_prompts.csv'):
    """åˆ†æç”Ÿæˆçš„æç¤ºè¯æ•°æ®é›†"""
    try:
        df = pd.read_csv(filename)
    except FileNotFoundError:
        print(f"\nâš ï¸  æ–‡ä»¶ {filename} ä¸å­˜åœ¨ã€‚è¯·å…ˆè¿è¡Œ generate_prompts.py")
        return None
    
    print("\n" + "="*80)
    print("ğŸ”„ ç”Ÿæˆæç¤ºè¯æ•°æ®é›†åˆ†æ / Generated Prompts Dataset Analysis")
    print("="*80)
    
    # åŸºæœ¬ç»Ÿè®¡
    print("\nğŸ“ˆ åŸºæœ¬ç»Ÿè®¡ / Basic Statistics:")
    print(f"   æ€»æç¤ºè¯æ•° / Total Prompts: {len(df)}")
    
    # æŒ‰ç±»å‹ç»Ÿè®¡
    print("\nğŸ“‚ æŒ‰ç±»å‹ç»Ÿè®¡ / Statistics by Type:")
    print("-" * 80)
    print(f"{'Type':<30} {'Count':<10} {'Percentage':<12}")
    print("-" * 80)
    
    for ptype, count in df['type'].value_counts().items():
        percentage = count / len(df) * 100
        print(f"{ptype:<30} {count:<10} {percentage:>6.1f}%")
    
    print("-" * 80)
    
    # æŒ‰é¢„æœŸæ ‡ç­¾ç»Ÿè®¡
    print("\nğŸ·ï¸  æŒ‰é¢„æœŸæ ‡ç­¾ç»Ÿè®¡ / Statistics by Expected Label:")
    print("-" * 80)
    print(f"{'Expected Label':<30} {'Count':<10} {'Percentage':<12}")
    print("-" * 80)
    
    for label, count in df['expected_label'].value_counts().items():
        percentage = count / len(df) * 100
        print(f"{label:<30} {count:<10} {percentage:>6.1f}%")
    
    print("-" * 80)
    
    # æç¤ºè¯é•¿åº¦åˆ†æ
    print("\nğŸ“ æç¤ºè¯é•¿åº¦åˆ†æ / Prompt Length Analysis:")
    df['length'] = df['prompt'].str.len()
    df['word_count'] = df['prompt'].str.split().str.len()
    
    print(f"   å¹³å‡å­—ç¬¦æ•° / Avg Characters: {df['length'].mean():.0f}")
    print(f"   å¹³å‡è¯æ•° / Avg Words: {df['word_count'].mean():.0f}")
    print(f"   æœ€çŸ­ / Min: {df['length'].min()} å­—ç¬¦")
    print(f"   æœ€é•¿ / Max: {df['length'].max()} å­—ç¬¦")
    
    print("\n   æŒ‰ç±»å‹åˆ†ç»„ / By Type:")
    for ptype in df['type'].unique():
        type_df = df[df['type'] == ptype]
        print(f"   {ptype:<25} - å¹³å‡: {type_df['length'].mean():.0f} å­—ç¬¦, {type_df['word_count'].mean():.0f} è¯")
    
    # åˆ†æç»„åˆè¿è§„
    print("\nğŸ”— ç»„åˆè¿è§„åˆ†æ / Combined Violations Analysis:")
    combined_df = df[df['type'].isin(['combined_bad', 'category_specific'])]
    
    if len(combined_df) > 0:
        details_list = combined_df['details'].apply(json.loads)
        
        # ç»Ÿè®¡è¿è§„æ•°é‡åˆ†å¸ƒ
        violation_counts = []
        for details in details_list:
            if 'num_violations' in details:
                violation_counts.append(details['num_violations'])
        
        if violation_counts:
            counter = Counter(violation_counts)
            print("\n   è¿è§„æ•°é‡åˆ†å¸ƒ / Violation Count Distribution:")
            for num, count in sorted(counter.items()):
                print(f"   {num} ä¸ªè¿è§„ / violations: {count} ä¸ªæç¤ºè¯ ({count/len(violation_counts)*100:.1f}%)")
    
    return df

def compare_datasets(seed_df, generated_df):
    """æ¯”è¾ƒç§å­æ•°æ®é›†å’Œç”Ÿæˆæ•°æ®é›†"""
    if generated_df is None:
        return
    
    print("\n" + "="*80)
    print("ğŸ” æ•°æ®é›†å¯¹æ¯” / Dataset Comparison")
    print("="*80)
    
    print("\nğŸ“Š æ•°é‡å¯¹æ¯” / Quantity Comparison:")
    print(f"   ç§å­æç¤ºè¯ / Seed Prompts: {len(seed_df)}")
    print(f"   ç”Ÿæˆæç¤ºè¯ / Generated Prompts: {len(generated_df)}")
    print(f"   æ€»è®¡ / Total: {len(seed_df) + len(generated_df)}")
    print(f"   å¢é•¿ç‡ / Growth Rate: {len(generated_df)/len(seed_df)*100:.1f}%")
    
    print("\nğŸ“ é•¿åº¦å¯¹æ¯” / Length Comparison:")
    seed_df['length'] = seed_df['prompt'].str.len()
    generated_df['length'] = generated_df['prompt'].str.len()
    
    print(f"   ç§å­æç¤ºè¯å¹³å‡é•¿åº¦ / Seed Avg: {seed_df['length'].mean():.0f} å­—ç¬¦")
    print(f"   ç”Ÿæˆæç¤ºè¯å¹³å‡é•¿åº¦ / Generated Avg: {generated_df['length'].mean():.0f} å­—ç¬¦")
    print(f"   å·®å¼‚ / Difference: {generated_df['length'].mean() - seed_df['length'].mean():.0f} å­—ç¬¦ "
          f"({(generated_df['length'].mean() / seed_df['length'].mean() - 1) * 100:+.1f}%)")

def export_summary(seed_df, generated_df, output_file='dataset_summary.json'):
    """å¯¼å‡ºæ•°æ®é›†æ‘˜è¦"""
    summary = {
        'seed_prompts': {
            'total': len(seed_df),
            'good': len(seed_df[seed_df['label'] == 'good']),
            'bad': len(seed_df[seed_df['label'] == 'bad']),
            'categories': seed_df['category'].nunique(),
            'subcategories': seed_df['subcategory'].nunique(),
            'avg_length': int(seed_df['prompt'].str.len().mean()),
            'by_category': {}
        }
    }
    
    # æŒ‰åˆ†ç±»ç»Ÿè®¡
    for category in seed_df['category'].unique():
        cat_df = seed_df[seed_df['category'] == category]
        summary['seed_prompts']['by_category'][category] = {
            'total': len(cat_df),
            'good': len(cat_df[cat_df['label'] == 'good']),
            'bad': len(cat_df[cat_df['label'] == 'bad'])
        }
    
    # ç”Ÿæˆæç¤ºè¯ç»Ÿè®¡
    if generated_df is not None:
        summary['generated_prompts'] = {
            'total': len(generated_df),
            'avg_length': int(generated_df['prompt'].str.len().mean()),
            'by_type': {}
        }
        
        for ptype in generated_df['type'].unique():
            type_df = generated_df[generated_df['type'] == ptype]
            summary['generated_prompts']['by_type'][ptype] = {
                'count': len(type_df),
                'percentage': round(len(type_df) / len(generated_df) * 100, 1)
            }
    
    # æ€»è®¡
    total_prompts = len(seed_df) + (len(generated_df) if generated_df is not None else 0)
    summary['total_dataset'] = {
        'total_prompts': total_prompts,
        'seed_prompts': len(seed_df),
        'generated_prompts': len(generated_df) if generated_df is not None else 0
    }
    
    # ä¿å­˜
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ æ•°æ®é›†æ‘˜è¦å·²ä¿å­˜åˆ°: {output_file}")

def main():
    print("\n" + "="*80)
    print("ğŸ”¬ æ•°æ®é›†åˆ†æå·¥å…· / Dataset Analysis Tool")
    print("="*80)
    
    # åˆ†æç§å­æç¤ºè¯
    seed_df = analyze_seed_prompts('seed_prompts.csv')
    
    # åˆ†æç”Ÿæˆçš„æç¤ºè¯
    generated_df = analyze_generated_prompts('generated_prompts.csv')
    
    # å¯¹æ¯”æ•°æ®é›†
    compare_datasets(seed_df, generated_df)
    
    # å¯¼å‡ºæ‘˜è¦
    export_summary(seed_df, generated_df, 'dataset_summary.json')
    
    print("\n" + "="*80)
    print("âœ… åˆ†æå®Œæˆï¼/ Analysis Complete!")
    print("="*80)
    
    print("\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶ / Generated Files:")
    print("   - dataset_summary.json (æ•°æ®é›†æ‘˜è¦)")
    
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®® / Usage Suggestions:")
    print("   1. ä½¿ç”¨ç§å­æ•°æ®è®­ç»ƒåŸºç¡€æ¨¡å‹")
    print("   2. ä½¿ç”¨ç”Ÿæˆæ•°æ®æµ‹è¯•æ¨¡å‹é²æ£’æ€§")
    print("   3. å®šæœŸè¿è¡Œ generate_prompts.py ç”Ÿæˆæ–°çš„æµ‹è¯•æ¡ˆä¾‹")
    print("   4. æ ¹æ®æ¨¡å‹è¡¨ç°è°ƒæ•´ç§å­æ•°æ®")

if __name__ == "__main__":
    main()




