#!/usr/bin/env python3
"""
Compute binary classification metrics (binary accuracy suite) from benchmark results.

The script expects one or more `benchmark_results_*.json` files generated by benchmark.py.
It aggregates results per model and prints a leaderboard with:
    - Accuracy
    - Precision/Recall/F1 for PASS (benign) and FAIL (violation) classes
    - Macro F1
    - Balanced accuracy
    - False positive rate (PASS misclassified as FAIL)
    - False negative rate (FAIL misclassified as PASS)
"""

from __future__ import annotations

import argparse
import json
import math
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, Iterable, List, Optional


POS_LABEL = "bad"   # FAIL prompts (violations)
NEG_LABEL = "good"  # PASS prompts (benign)


@dataclass
class ConfusionMatrix:
    tp: int = 0
    tn: int = 0
    fp: int = 0
    fn: int = 0

    def update(self, truth: str, pred: str) -> None:
        if truth == POS_LABEL:
            if pred == POS_LABEL:
                self.tp += 1
            else:
                self.fn += 1
        elif truth == NEG_LABEL:
            if pred == NEG_LABEL:
                self.tn += 1
            else:
                self.fp += 1

    @property
    def total(self) -> int:
        return self.tp + self.tn + self.fp + self.fn

    def precision(self) -> float:
        denom = self.tp + self.fp
        return self.tp / denom if denom else math.nan

    def recall(self) -> float:
        denom = self.tp + self.fn
        return self.tp / denom if denom else math.nan

    def f1(self) -> float:
        p = self.precision()
        r = self.recall()
        if math.isnan(p) or math.isnan(r) or (p + r) == 0:
            return math.nan
        return 2 * p * r / (p + r)

    def specificity(self) -> float:
        denom = self.tn + self.fp
        return self.tn / denom if denom else math.nan

    def accuracy(self) -> float:
        return (self.tp + self.tn) / self.total if self.total else math.nan

    def fpr(self) -> float:
        denom = self.fp + self.tn
        return self.fp / denom if denom else math.nan

    def fnr(self) -> float:
        denom = self.fn + self.tp
        return self.fn / denom if denom else math.nan


@dataclass
class BinaryMetrics:
    model_name: str
    confusion: ConfusionMatrix
    neg_precision: float
    neg_recall: float
    neg_f1: float

    def to_row(self) -> Dict[str, Optional[float]]:
        return {
            "model_name": self.model_name,
            "accuracy": self.confusion.accuracy(),
            "pos_precision": self.confusion.precision(),
            "pos_recall": self.confusion.recall(),
            "pos_f1": self.confusion.f1(),
            "neg_precision": self.neg_precision,
            "neg_recall": self.neg_recall,
            "neg_f1": self.neg_f1,
            "macro_f1": safe_mean([self.confusion.f1(), self.neg_f1]),
            "balanced_accuracy": safe_mean([self.confusion.recall(), self.confusion.specificity()]),
            "false_positive_rate": self.confusion.fpr(),
            "false_negative_rate": self.confusion.fnr(),
            "tp": self.confusion.tp,
            "tn": self.confusion.tn,
            "fp": self.confusion.fp,
            "fn": self.confusion.fn,
        }


def safe_mean(values: Iterable[Optional[float]]) -> float:
    clean = [v for v in values if v is not None and not math.isnan(v)]
    if not clean:
        return math.nan
    return sum(clean) / len(clean)


def load_results(paths: List[Path]) -> Dict[str, List[dict]]:
    per_model: Dict[str, List[dict]] = {}
    for path in paths:
        with path.open("r", encoding="utf-8") as fh:
            payload = json.load(fh)
        for entry in payload.get("results", []):
            model_id = entry.get("model_id")
            if not model_id:
                continue
            per_model.setdefault(model_id, []).append(entry)
    return per_model


def compute_metrics(model_results: Dict[str, List[dict]]) -> List[BinaryMetrics]:
    metrics: List[BinaryMetrics] = []
    for model_id, records in model_results.items():
        confusion = ConfusionMatrix()
        neg_tp = neg_fp = neg_fn = neg_tn = 0
        model_name = None

        for record in records:
            if record.get("status") != "success":
                continue
            model_name = record.get("model_name") or model_id

            truth = normalize_truth(record.get("prompt_type") or record.get("prompt_meta", {}).get("prompt_type"))
            if truth not in {POS_LABEL, NEG_LABEL}:
                continue

            prediction = normalize_prediction(record.get("overall_status"))
            confusion.update(truth, prediction)

            # For PASS metrics treat PASS as positive in a separate confusion matrix
            if truth == NEG_LABEL:
                if prediction == NEG_LABEL:
                    neg_tp += 1  # true PASS
                else:
                    neg_fn += 1  # PASS predicted FAIL (false positive)
            else:
                if prediction == NEG_LABEL:
                    neg_fp += 1  # FAIL predicted PASS (false negative for PASS class)
                else:
                    neg_tn += 1

        neg_precision = neg_tp / (neg_tp + neg_fp) if (neg_tp + neg_fp) else math.nan
        neg_recall = neg_tp / (neg_tp + neg_fn) if (neg_tp + neg_fn) else math.nan
        if math.isnan(neg_precision) or math.isnan(neg_recall) or (neg_precision + neg_recall) == 0:
            neg_f1 = math.nan
        else:
            neg_f1 = 2 * neg_precision * neg_recall / (neg_precision + neg_recall)

        metrics.append(BinaryMetrics(
            model_name=model_name or model_id,
            confusion=confusion,
            neg_precision=neg_precision,
            neg_recall=neg_recall,
            neg_f1=neg_f1,
        ))
    return metrics


def normalize_truth(label: Optional[str]) -> Optional[str]:
    if not label:
        return None
    label = label.lower()
    if label in {"bad", "fail"}:
        return POS_LABEL
    if label in {"good", "pass"}:
        return NEG_LABEL
    return None


def normalize_prediction(status: Optional[str]) -> Optional[str]:
    if not status:
        return None
    status = status.upper()
    if status == "FAIL":
        return POS_LABEL
    if status == "PASS":
        return NEG_LABEL
    return None


def format_float(value: Optional[float]) -> str:
    if value is None or math.isnan(value):
        return "  N/A"
    return f"{value * 100:6.2f}%"


def build_leaderboard(metrics: List[BinaryMetrics], top: Optional[int]) -> str:
    header = (
        f"{'Rank':>4}  {'Model':<28}  {'Accuracy':>9}  {'FAIL F1':>9}  {'PASS F1':>9}  "
        f"{'Macro F1':>9}  {'Bal.Acc':>9}  {'FPR':>7}  {'FNR':>7}"
    )
    rows = [header, "-" * len(header)]

    sorted_metrics = sorted(
        metrics,
        key=lambda m: (
            sanitize_sort_value(m.to_row()["macro_f1"]),
            sanitize_sort_value(m.to_row()["accuracy"]),
        ),
        reverse=True,
    )

    if top is not None:
        sorted_metrics = sorted_metrics[:top]

    for idx, metric in enumerate(sorted_metrics, start=1):
        row = metric.to_row()
        rows.append(
            f"{idx:>4}  {metric.model_name:<28}  "
            f"{format_float(row['accuracy']):>9}  "
            f"{format_float(row['pos_f1']):>9}  "
            f"{format_float(row['neg_f1']):>9}  "
            f"{format_float(row['macro_f1']):>9}  "
            f"{format_float(row['balanced_accuracy']):>9}  "
            f"{format_float(row['false_positive_rate']):>7}  "
            f"{format_float(row['false_negative_rate']):>7}"
        )
    return "\n".join(rows)


def sanitize_sort_value(value: Optional[float]) -> float:
    if value is None or math.isnan(value):
        return float("-inf")
    return value


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Compute binary accuracy leaderboard from benchmark results.")
    parser.add_argument(
        "--results",
        nargs="+",
        type=Path,
        help="One or more benchmark_results_*.json files.",
    )
    parser.add_argument(
        "--top",
        type=int,
        default=None,
        help="Show only the top N models.",
    )
    parser.add_argument(
        "--show-counts",
        action="store_true",
        help="Print confusion matrix counts for each model.",
    )
    return parser.parse_args()


def main() -> int:
    args = parse_args()
    if not args.results:
        print("Please specify at least one results file with --results.")
        return 1

    model_results = load_results(args.results)
    if not model_results:
        print("No model results found in the provided files.")
        return 1

    metrics = compute_metrics(model_results)
    print(build_leaderboard(metrics, args.top))

    if args.show_counts:
        print("\nConfusion matrix counts (FAIL positive / PASS negative):")
        for metric in metrics:
            row = metric.to_row()
            print(
                f"- {metric.model_name}: "
                f"TP={row['tp']}, FN={row['fn']}, FP={row['fp']}, TN={row['tn']}"
            )

    return 0


if __name__ == "__main__":
    raise SystemExit(main())




